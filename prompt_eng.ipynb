{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Prompt Engineering\n",
    "\n",
    "Prompt engineering is using techniques when writing prompts to get better, and more consistent, responses from AI models like ChatGPT and Claude.  The more complex task the more benefit you will get from prompt engineering.\n",
    "\n",
    "\n",
    "**Prompt engineering techniques:**\n",
    "\n",
    "- give clear instructions\n",
    "- provide examples\n",
    "- set a persona\n",
    "- start a new chat if things go wrong\n",
    "\n",
    "**Advanced techniques:**\n",
    "\n",
    "- XML tagging\n",
    "- Give the model time to think/ Break up complex tasks\n",
    "- Use format support for output consistency\n",
    "- Prefilled responses\n",
    "- Cater for model context limits\n",
    "\n",
    "**Links:**\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering\n",
    "\n",
    "https://docs.anthropic.com/claude/docs/prompt-engineering\n",
    "\n",
    "https://docs.ell.so/\n",
    "\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "For this session you will need an OpenAI account, and an OpenAI API token setup. https://platform.openai.com/settings then click on \"API keys\" where you can create a new key.\n",
    "\n",
    "You will need a .env file in the project directory with the following line filled out\n",
    "> OPENAI_API_KEY = \\<your-openai-api-key\\>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install openai --upgrade\n",
    "%pip install -U ell-ai\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have issues install the above packages with a message about a missing 'typing_extgensions' package then try running these on the command line and then try again\n",
    "```\n",
    "pip install --upgrade pip\n",
    "pip install typing-extensions\n",
    "pip install ipykernel --upgrade --force-reinstall\n",
    "python -m ipykernel install --user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give clear instructions\n",
    "\n",
    "Give details, be specific, assume that the model doesn't know what you are talking about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cheapest car to own in Australia can vary based on factors such as purchase price, fuel efficiency, maintenance costs, insurance, and depreciation. As of the latest available information, the Suzuki Alto, Kia Picanto, and Mitsubishi Mirage are often cited as some of the cheapest cars to own due to their low purchase prices and good fuel economy.\n",
      "\n",
      "When determining the cheapest car to own, it's important to consider not only the initial purchase price but also ongoing expenses. It's advisable to look at annual running costs and consult recent reviews or cost comparisons from trusted Australian automotive websites or financial institutions that analyze these factors comprehensively.\n"
     ]
    }
   ],
   "source": [
    "# Simple function that sends ChatGPT user prompt requests\n",
    "def simple_prompt(prompt):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Examples of being more specific in your prompting \n",
    "\"\"\"\n",
    "my_prompt = \"what is the cheapest car to own in Australia?\"\n",
    "# my_prompt = \"what is the cheapest petrol car to own in Australia? The car model should not be more than 5 years old and have a 5 star safety rating. Exclude SUVs or anything with 4WD. Give the estimated price in Australian dollars and the year of manufacture\"\n",
    "\n",
    "# my_prompt = \"Who was Australia‚Äôs greatest prime minister?\"\n",
    "# my_prompt = \"Who was Australia‚Äôs greatest prime minister?  The answer is subjective but if you had to pick one who would it be and why?\"\n",
    "\n",
    "print(simple_prompt(my_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give examples\n",
    "\n",
    "LLM's **LOVE** examples. \n",
    "\n",
    "This could include:\n",
    "- examples on how you want the model to repond\n",
    "- example on how you want the reponse to be formatted (e.g. json) **\n",
    "\n",
    "** this can also be done througn Pydantic Python\n",
    "\n",
    "\n",
    "### Number of examples given: \n",
    "\n",
    "The more examples you give the better, and this technique has names.\n",
    "\n",
    "**Zero-shot:**\n",
    "- no examples given\n",
    "- simple but less accurate for complex tasks\n",
    "\n",
    "**One-shot:**\n",
    "- one example\n",
    "- helps model understand required response/format\n",
    "- even one example is significantly better than none\n",
    "\n",
    "**Few-shot/Many-shot:**\n",
    "- multiple examples given\n",
    "- better performance as more context for model to work with\n",
    "- excels for use with complex tasks/specific formats (input and/or output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2022, the top Australian female tennis players, along with their win/loss records and estimated prize money, are as follows:\n",
      "\n",
      "1. Ashleigh Barty, ranking: 1st, prize money: Approximately $2,500,000, win/loss: 14/1\n",
      "2. Ajla Tomljanoviƒá, ranking: 33rd, prize money: Approximately $1,026,000, win/loss: 36/26\n",
      "3. Daria Saville, ranking: 53rd, prize money: Approximately $600,000, win/loss: 23/18\n",
      "\n",
      "Note: Rankings, prize money, and win/loss records can vary slightly depending on the source, as these figures are estimated based on available data.\n"
     ]
    }
   ],
   "source": [
    "# Example of defining the output you want to receive\n",
    "\n",
    "my_prompt = \"\"\" You are an expert in sports statistics. List the top three Australian female tennis players in 2022. Include their win/loss results and total estimated prise money for that year.\n",
    "Output results in a numbered list with the following format:\n",
    "John McEnroe, ranking:3rd, prize money: $1,234,000, win/loss: 45/13\n",
    "\"\"\"\n",
    "\n",
    "print(simple_prompt(my_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-shot: Positive\n",
      "**************************************************\n",
      "one-shot: Positive\n",
      "**************************************************\n",
      "many-shot: Positive\n"
     ]
    }
   ],
   "source": [
    "# Define a function that lets us specify a set of messages for our prompt\n",
    "def complex_prompt(messages):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "# Example of giving no, one, or many examples in your prompt\n",
    "\n",
    "input_text = \"Love this thing\"\n",
    "#input_text = \"did what it said it would do\"\n",
    "#input_text = \"Mostly I liked it, but the price seemed difficult to accept\"\n",
    "#input_text = \"it's a cracker\"\n",
    "\n",
    "\n",
    "# No-shot prompting - just the task with no examples\n",
    "no_shot_msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify this text as either positive or negative: '{input_text}'\"}\n",
    "]\n",
    "\n",
    "# One-shot prompting - includes a single example\n",
    "one_shot_msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Classify this text as either positive or negative: 'I love this product!'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Positive\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify this text as either positive or negative: '{input_text}'\"}\n",
    "]\n",
    "\n",
    "# Many-shot prompting - includes multiple examples\n",
    "many_shot_msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Classify this text as either positive or negative: 'I love this product!'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Positive\"},\n",
    "    {\"role\": \"user\", \"content\": \"Classify this text as either positive or negative: 'This is terrible.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Negative\"},\n",
    "    {\"role\": \"user\", \"content\": \"Classify this text as either positive or negative: 'It's okay, nothing special.'\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Positive\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Classify this text as either positive or negative: '{input_text}'\"}\n",
    "]\n",
    "\n",
    "print(f\"no-shot: {complex_prompt(no_shot_msgs)}\")\n",
    "print(\"*\" * 50)  # Prints 50 asterisks in a row\n",
    "print(f\"one-shot: {complex_prompt(one_shot_msgs)}\")\n",
    "print(\"*\" * 50)  # Prints 50 asterisks in a row\n",
    "print(f\"many-shot: {complex_prompt(many_shot_msgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a persona or role\n",
    "\n",
    "Setting a persona gives additional context to the response that should be generated.  Likewise you could describe the role or task the model is responsible for completing.\n",
    "\n",
    "Examples:\n",
    "- \"You are an expert in \\<subject\\>\" (marketing, HR, English grammer, etc), or \"You are a \\<position\\>\" (history teacher, mathematician, dietician, etc)\n",
    "- \"Your role is to \\<role/task\\> (identify common trends in datasets, make recommendations to improve processes, review content for grammar and appropriate reading comprehension levels, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! A successful social media campaign requires careful planning, execution, and analysis. Here are the top five activities marketers should focus on:\n",
      "\n",
      "1. **Define Clear Objectives**:\n",
      "   - Establish what you want to achieve with your campaign, such as increasing brand awareness, driving website traffic, generating leads, or boosting sales. Clear objectives will guide your strategy and help measure the success of your campaign.\n",
      "\n",
      "2. **Know Your Audience**:\n",
      "   - Conduct thorough audience research to understand the demographics, preferences, behaviors, and pain points of your target audience. This information will help tailor your content and messaging to resonate with them effectively.\n",
      "\n",
      "3. **Create Engaging and Relevant Content**:\n",
      "   - Develop high-quality content that aligns with your brand and appeals to your audience. Use a mix of formats such as images, videos, infographics, and live streams to keep your audience engaged. Ensure that the content is authentic and provides value to your audience.\n",
      "\n",
      "4. **Optimize for Each Platform**:\n",
      "   - Different social media platforms have distinct features and user demographics. Tailor your content and messaging to fit the platform you‚Äôre using. Utilize platform-specific tools and features, such as Instagram Stories or LinkedIn Articles, to enhance engagement and visibility.\n",
      "\n",
      "5. **Monitor and Adjust**:\n",
      "   - Regularly track the performance of your campaign using analytics tools to measure key metrics such as reach, engagement, conversion rates, and ROI. Be prepared to adjust your strategy and tactics based on data insights to maximize campaign effectiveness and achieve your goals.\n",
      "\n",
      "By focusing on these activities, marketers can create impactful social media campaigns that drive meaningful results for their brands.\n"
     ]
    }
   ],
   "source": [
    "my_prompt = \"You are an expert in marketing.  List the top 5 activities marketers should do to have a successful social media campaign.\"\n",
    "\n",
    "# my_prompt = \"You are an English woman born in the early 1900's. Describe your rights under the English legal system. Summarise in a bulleted list of no more than 5 points\"\n",
    "\n",
    "print(simple_prompt(my_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start a new chat\n",
    "\n",
    "If things go wrong start new chat to avoid previous requests/responses being including in your prompt.  If using OpenAI assistants consider starting a new thread (threads automatically provide previous message history to the assistant).  \n",
    "\n",
    "People will often talk of previous message history 'polluting' the current request and causing unwanted results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now onto more advanced techniques.\n",
    "\n",
    "Advanced prompt engineering techniques should be used when:\n",
    "\n",
    "- implementing complex prompts \n",
    "\n",
    "- we require specific and consistently formatted responses (e.g. JSON, etc).\n",
    "\n",
    "\n",
    "# XML Tagging\n",
    "\n",
    "To help the model to identify the different parts of a prompt (like where an example starts/ends) we can use delimters.  One useful type of delimiters are xml tags (e.g.\"\\<code\\>...\\</code\\>\").  The name of text in the tag doesn't matter, although giving it a meaningful name will probably help (like \"example\", \"code\", \"format\", etc).\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"title\": \"The Matrix\",\n",
      "    \"year\": 1999,\n",
      "    \"director\": \"The Wachowskis\",\n",
      "    \"rating\": 8.7,\n",
      "    \"genres\": [\"Sci-Fi\", \"Action\"],\n",
      "    \"mainCast\": [\n",
      "        {\n",
      "            \"name\": \"Keanu Reeves\",\n",
      "            \"role\": \"Neo\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Laurence Fishburne\",\n",
      "            \"role\": \"Morpheus\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Carrie-Anne Moss\",\n",
      "            \"role\": \"Trinity\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Hugo Weaving\",\n",
      "            \"role\": \"Agent Smith\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Here we specifically identify the example and instruction text for the model.\n",
    "# There are better ways of getting models to return JSON formatted responses but\n",
    "# it is a good example of clearly defining the different parts of the prompt.\n",
    "\n",
    "movie_title = \"Terminator 2\"\n",
    "\n",
    "my_prompt = \"\"\"\n",
    "    Provide information about the movie in JSON format. \n",
    "    \n",
    "    <example>\n",
    "    {\n",
    "        \"title\": \"The Matrix\",\n",
    "        \"year\": 1999,\n",
    "        \"director\": \"The Wachowskis\",\n",
    "        \"rating\": 8.7,\n",
    "        \"genres\": [\"Sci-Fi\", \"Action\"],\n",
    "        \"mainCast\": [\n",
    "            {\n",
    "                \"name\": \"Keanu Reeves\",\n",
    "                \"role\": \"Neo\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Laurence Fishburne\",\n",
    "                \"role\": \"Morpheus\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    </example>\n",
    "\n",
    "    <instructions>\n",
    "    Follow the exact same JSON structure as shown in the example above.\n",
    "    Ensure all field names match exactly.\n",
    "    Include only factual information about the movie.\n",
    "    Include 3-5 cast members.\n",
    "    </instructions>\n",
    "\n",
    "    Provide information for the movie: {movie_title}\n",
    "    \"\"\"\n",
    "    \n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a movie database expert. Always return valid JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": my_prompt}\n",
    "]\n",
    "\n",
    "response = complex_prompt(messages)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give the model time to think \n",
    "\n",
    "One of the issues with LLM's is the importance of being responsive, in other words don't take too long to give a response.  However we may not care about how long the request takes if it means we get a better response.  Very recently OpenAI release o1 which now prioritises taking its time, breaking tasks into steps, and returning more meaningful results.\n",
    "\n",
    "The key phrase to use in your prompt is \"step by step\".  Due to the way LLM's work you will always see evidence of the workings of a step, before it is feed into the next step, i.e. you will receive intermediate results before you get the final response.\n",
    "\n",
    "https://docs.anthropic.com/claude/docs/let-claude-think\n",
    "\n",
    "\n",
    "An alternative approach to to break you request into multiple smaller requests, feeding the output of previous request into the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the problem, we'll go through it step-by-step:\n",
      "\n",
      "1. **Initial Number of Tennis Balls:**  \n",
      "   Bob starts with 2 tennis balls.\n",
      "\n",
      "2. **Number of Cans Bought:**  \n",
      "   Bob buys 2 cans.\n",
      "\n",
      "3. **Tennis Balls Per Can:**  \n",
      "   Each can contains 3 tennis balls.\n",
      "\n",
      "4. **Calculating Total Balls from the Cans:**  \n",
      "   Since Bob buys 2 cans and each can contains 3 tennis balls, we calculate the total number of tennis balls in the cans as follows:\n",
      "   \\[\n",
      "   \\text{Total tennis balls from cans} = \\text{Number of cans} \\times \\text{Tennis balls per can} = 2 \\times 3 = 6\n",
      "   \\]\n",
      "\n",
      "5. **Total Tennis Balls Overall:**  \n",
      "   Finally, to find the total number of tennis balls Bob has, we add the number of tennis balls he originally had to the number of tennis balls from the cans:\n",
      "   \\[\n",
      "   \\text{Total tennis balls} = \\text{Initial tennis balls} + \\text{Total tennis balls from cans} = 2 + 6 = 8\n",
      "   \\]\n",
      "\n",
      "Therefore, Bob now has 8 tennis balls.\n"
     ]
    }
   ],
   "source": [
    "# Example of telling the model to take its time in generating the response\n",
    "\n",
    "my_prompt = f\"\"\"\n",
    "    Bob has 2 tennis balls.  Be buys 2 more cans of tennis ball, each can holds 3 tennis balls.  How many tennis balls does Bob have now?\n",
    "    Take your time. Think about this step-by-step and show all your work.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a methodical problem solver who breaks down complex problems into manageable steps.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": my_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "response = complex_prompt(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakup complex tasks\n",
    "\n",
    "AI models can stuggle with large complex prompts and benefit from splitting them into smaller less complex tasks.  This can be approached two ways:\n",
    "\n",
    "- Single task - maintain having a single prompt but give the AI a number of smaller steps to follow in constructing the response.\n",
    "\n",
    "- Multiple tasks - break the prompt into a number of smaller, separate, prompts passing data between them were necessary (this technique is called ‚Äúprompt chaining‚Äù)\n",
    "\n",
    "Keep in mind large complex prompts may hit issues from the amount of time needed to process them (outlined in ‚ÄúGive the model time to think‚Äù). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, we'll follow the instructions provided by breaking it into smaller parts and addressing each one methodically. Here is the complete breakdown:\n",
      "\n",
      "### Step 1: Apply the Sale Discount\n",
      "\n",
      "1. **Calculate the Discounted Price of Each Item:**\n",
      "\n",
      "   - **Shirt:**\n",
      "     - Original price = $80\n",
      "     - Discount = 20% of $80\n",
      "     - Discount = \\( 0.20 \\times 80 = 16 \\)\n",
      "     - Discounted price of shirt = $80 - $16 = $64\n",
      "\n",
      "   - **Pants:**\n",
      "     - Original price = $120\n",
      "     - Discount = 20% of $120\n",
      "     - Discount = \\( 0.20 \\times 120 = 24 \\)\n",
      "     - Discounted price of pants = $120 - $24 = $96\n",
      "\n",
      "2. **Calculate the Total Price After Discounts:**\n",
      "   - Total discounted price = $64 (shirt) + $96 (pants) = $160\n",
      "\n",
      "### Step 2: Apply the $25 Coupon\n",
      "\n",
      "1. **Subtract the Coupon Value from the Total Discounted Price:**\n",
      "\n",
      "   - Total after coupon = $160 - $25 = $135\n",
      "\n",
      "### Step 3: Calculate Sales Tax\n",
      "\n",
      "1. **Determine the Sales Tax on the Total After the Coupon:**\n",
      "   - Sales tax rate = 8%\n",
      "   - Sales tax = \\( 0.08 \\times 135 = 10.8 \\)\n",
      "\n",
      "### Step 4: Calculate the Final Total\n",
      "\n",
      "1. **Add the Sales Tax to the Total After the Coupon:**\n",
      "   - Final total = $135 + $10.8 = $145.8\n",
      "\n",
      "### Step 5: Verification\n",
      "\n",
      "To ensure accuracy, let's verify as follows:\n",
      "\n",
      "- **Price of Shirt After Discount:** $64 (Checked)\n",
      "- **Price of Pants After Discount:** $96 (Checked)\n",
      "- **Total after Discounts and Coupon:** $135 (Checked)\n",
      "- **Sales Tax Calculation:** 8% of $135 is $10.8 (Checked)\n",
      "- **Final Total Calculation:** $135 + $10.8 = $145.8 (Checked)\n",
      "\n",
      "### Assumptions\n",
      "\n",
      "- The 20% discount applies separately to each item before combining their prices.\n",
      "- The $25 coupon is applied after the items are discounted and the total is calculated.\n",
      "- Sales tax is applied on the amount after the coupon has been deducted from the discounted prices.\n",
      "\n",
      "The final total price, including sales tax, is **$145.80**.\n"
     ]
    }
   ],
   "source": [
    "# Example defining the steps you want the model to follow in a single request\n",
    "my_prompt = f\"\"\"\n",
    "    I want you to solve this problem carefully and methodically. \n",
    "    \n",
    "    <instructions>\n",
    "    1. First, break down the problem into smaller parts\n",
    "    2. For each part, explain your thinking process\n",
    "    3. Show your work at each step\n",
    "    4. Before giving the final answer, verify your solution\n",
    "    5. If you make any assumptions, state them explicitly\n",
    "    </instructions>\n",
    "\n",
    "    <problem>\n",
    "    A store is having a 20% off sale. If you buy a shirt that normally costs $80 \n",
    "    and pants that normally cost $120, and you have a $25 coupon that applies after \n",
    "    the sale discount, what is your final total including 8% sales tax?\n",
    "    </problem>\n",
    "\n",
    "    Take your time. Think about this step-by-step and show all your work.\n",
    "    \"\"\"\n",
    "    \n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a methodical problem solver who breaks down complex problems into manageable steps.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": my_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "response = complex_prompt(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use format support for output consistency\n",
    "\n",
    "Many LLM‚Äôs support different output formats like JSON, CSV, HTML, etc.  We can utililise this to ensure the output we receive from the LLM is in a consistent format for send to other jobs for processing.  This is extremely useful where we want to have the response in a consistant format we can pass to another process.\n",
    "\n",
    "- certain key phrases like \"JSON format\" to trigger the functionality\n",
    "- parameters in the request setting the format, like the response_format parameter in OpenAI API requests\n",
    "- use of methods like Pydantic Python to set a structure/schema that the LLM recognises and conforms to.\n",
    "\n",
    "(the later Ell example uses Pydantic Python to define the output structure)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefilled responses\n",
    "\n",
    "In this technique we have given the our user prompt as well as the start of the assitant response.  The model will start adding to the response it has been given, in this case \"Once up Once upon a time, in a land far away, there was...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far away, there was a brave knight named Sir Cedric. Known for his courage and unwavering sense of justice, Sir Cedric was beloved by all who dwelt in the kingdom of Eldoria.\n",
      "\n",
      "One fateful day, a fierce dragon descended upon the peaceful village of Brookshire, casting a shadow of fear across the land. The villagers, having never faced such a peril, sent for Sir Cedric with hopes that he could rid them of this dreadful creature.\n",
      "\n",
      "Without hesitation, Sir Cedric mounted his loyal steed, Onyx, and galloped towards the village. The journey was fraught with danger as the path wound through dark forests and steep mountains, yet he pressed on, unfazed. \n",
      "\n",
      "Upon reaching Brookshire, Sir Cedric saw the destruction the dragon had wrought. The villagers gathered around him, their eyes filled with both hope and terror. With a reassuring smile, he promised to protect them, and ventured into the dragon's lair alone.\n",
      "\n",
      "As he approached, the dragon emerged, its scales glinting like jewels in the sunlight. Sir Cedric stood firm, raising his shimmering sword, Excalibur. A fierce battle ensued; the dragon unleashed torrents of flame, but Sir Cedric was swift and agile, dodging the fiery blasts. \n",
      "\n",
      "Finally, he found an opening and struck with precision and strength. The dragon roared in pain and, realizing its defeat, flew away, never to trouble Brookshire again.\n",
      "\n",
      "The villagers hailed Sir Cedric as their hero, celebrating his bravery far and wide. Yet, in his modesty, he simply nodded and said, \"Fear not, for courage and goodness will always prevail.\"\n",
      "\n",
      "And with that, Sir Cedric continued his journey, ever watchful for those in need, his heart as strong as his unyielding sword, ever ready to face whatever challenges lay ahead.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a very short story about a brave knight.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Once upon a time, in a land far away, there was \"}\n",
    "]\n",
    "\n",
    "reponse = complex_prompt(messages)\n",
    "\n",
    "print(messages[1]['content'] + reponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cater for model context limits\n",
    "LLM's have a limit to the size of the request they can accept, and recently that limit has greatly increased (Claude 3.5 is now 200k tokens).  Hitting a context limit will either cause the request to error or the output to be truncated.  \n",
    "\n",
    "Remember that the size of your request isn't just the prompt you wrote, but also any content (files, etc) you have attached.  The model may also automatically add in previous message history as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Ell prompting framework\n",
    "\n",
    "Recently the Ell framework has been released to help with developing prompts.  A few of its useful features are the versioning of prompts, keeping a history of requests/results, and a graphical front end.\n",
    "\n",
    "Currently it supports the OpenAI and Anthropic APIs.  Here we are using the OpenAI API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ell\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "ell.init(store='./logdir', autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Sam Altman! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# traditional call using eh OpenAI API\n",
    "# How we usually call LLM in OpenAI API\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Say hello to Sam Altman!\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, Sam Altman! It's great to have you here. How can I assist you today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now using Ell\n",
    "\n",
    "@ell.simple(model=\"gpt-4o\")\n",
    "def hello(name: str):\n",
    "    \"\"\"You are a helpful assistant.\"\"\" # System prompt\n",
    "    return f\"Say hello to {name}!\" # User prompt\n",
    "\n",
    "hello(\"Sam Altman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë hello(Bobby)\n",
      "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
      "‚ïë Prompt:\n",
      "‚ïü‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï¢\n",
      "‚îÇ      system: You are a helpful assistant.\n",
      "‚îÇ\n",
      "‚îÇ        user: Say hello to Bobby!\n",
      "‚ïü‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï¢\n",
      "‚ïë Output:\n",
      "‚ïü‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï¢\n",
      "‚îÇ   assistant: Hello, Bobby! How are you today?\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "Hello, Bobby! How are you today?\n"
     ]
    }
   ],
   "source": [
    "# You can enable verbose output to get more behind the scenes detail\n",
    "\n",
    "ell.init(verbose=True)\n",
    "\n",
    "print(hello(\"Bobby\"))\n",
    "\n",
    "ell.init(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Sam Altman! It's fantastic to connect with you! üéâ\n"
     ]
    }
   ],
   "source": [
    "# Example of a more complex request with prefilled messages\n",
    "\n",
    "@ell.simple(model=\"gpt-4o\")\n",
    "def hello(name: str):\n",
    "    return [\n",
    "        ell.system(\"You are a helpful assistant.\"),\n",
    "        ell.user(f\"Say hello to {name}!\"),\n",
    "        ell.assistant(\"Hello! I'd be happy to greet Sam Altman.\"),\n",
    "        ell.user(\"Great! Now do it more enthusiastically.\")\n",
    "    ]\n",
    "\n",
    "greeting = hello(\"Sam Altman\")\n",
    "print(greeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, Sara Smith! üåü It's wonderful to meet you! I hope you're having an amazing day! üéâüòä\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As the prompt is effectively now a function we can easily call it\n",
    "hello(\"Sara Smith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waking up on time can be challenging, but with some strategies, you can make it easier. Here are five helpful tips:\n",
      "\n",
      "1. **Establish a Consistent Sleep Schedule**: Go to bed and wake up at the same time every day, even on weekends. This helps regulate your body's internal clock and can make it easier to wake up naturally.\n",
      "\n",
      "2. **Create a Bedtime Routine**: Develop a relaxing routine before bed to signal to your body that it's time to wind down. This could include reading a book, listening to calming music, or practicing meditation or deep breathing exercises.\n",
      "\n",
      "3. **Limit Exposure to Screens Before Bed**: The blue light emitted by phones, tablets, and computers can interfere with your body's production of melatonin, a hormone that regulates sleep. Try to avoid screens at least an hour before bedtime.\n",
      "\n",
      "4. **Optimize Your Sleep Environment**: Make sure your bedroom is conducive to sleep. Keep it cool, quiet, and dark, and invest in a comfortable mattress and pillows. Consider using blackout curtains and a white noise machine if needed.\n",
      "\n",
      "5. **Use an Effective Alarm System**: Place your alarm clock across the room, so you have to get out of bed to turn it off. Consider an alarm with a loud, pleasant sound, or use a smartphone app that requires you to solve a puzzle or take a photo of something specific to turn it off, ensuring you're fully awake by the time it's off.\n"
     ]
    }
   ],
   "source": [
    "# A slightly more useful example\n",
    "@ell.simple(model=\"gpt-4o\")\n",
    "def tips(subject: str):\n",
    "    return [\n",
    "        ell.system(\"You are a helpful assistant.\"),\n",
    "        ell.user(f\"List 5 helpful tips for {subject}\"),\n",
    "    ]\n",
    "\n",
    "response = tips(\"waking up on time\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here are five helpful tips for fixing cars:\n",
      "\n",
      "1. **Read the Manual:** Start by consulting your car's owner's manual. It contains valuable information about your vehicle's specific features and maintenance requirements. Understanding your car's layout and specifications will make diagnosing problems easier.\n",
      "\n",
      "2. **Gather the Right Tools:** Make sure you have a basic set of automotive tools, including wrenches, screwdrivers, pliers, and a socket set. Specialized tools may sometimes be necessary depending on the repair, so identify what you'll need before you start a job.\n",
      "\n",
      "3. **Diagnose the Problem Accurately:** Before attempting any repair, make sure you accurately diagnose the issue. This could involve checking error codes with an OBD-II scanner, listening for unusual sounds, or visually inspecting components for damage or wear.\n",
      "\n",
      "4. **Use Online Resources and Tutorials:** There are numerous online resources available, including forums, YouTube videos, and repair guide websites like Haynes or Chilton. These can provide detailed step-by-step instructions and visuals, which can be extremely helpful, especially for DIY repairs.\n",
      "\n",
      "5. **Prioritize Safety:** Always prioritize your safety when working on your car. Ensure the car is on a flat surface and use jack stands if you need to work underneath. Wear safety goggles when necessary, disconnect the battery to prevent electrical issues, and be cautious when dealing with fluids and chemicals.\n",
      "\n",
      "These tips can help make car repair more approachable and effective, whether you're addressing simple maintenance tasks or more complex issues.\n"
     ]
    }
   ],
   "source": [
    "print(tips(\"fixing cars\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start the visualisation front end by entering the following command in the terminal, then open the link shown in the output in your browser.\n",
    "\n",
    "> ell-studio --storage ./logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses Pydantic Python which allows us to specify the structure/schema of content.  Importantly LLMs can understand these structures and it becomes a very useful way for us to specify the format it should return the response in.\n",
    "\n",
    "The file movies.py has the layout of the response we want the LLM to return.\n",
    "\n",
    "https://docs.pydantic.dev/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Title: The Dirty Dozen\n",
      "Rating: 9/10\n",
      "Summary: The Dirty Dozen is a gripping World War II action film that follows a group of 12 convicted soldiers given a chance at redemption. Assembled for a suicide mission behind enemy lines, they must infiltrate a Nazi stronghold to eliminate high-rank German officers. With intense action sequences and a touch of dark humor, this classic delivers thrilling entertainment.\n",
      "Cast:\n",
      "\t- Lee Marvin\n",
      "\t- Ernest Borgnine\n",
      "\t- Charles Bronson\n",
      "\t- Jim Brown\n",
      "\t- John Cassavetes\n"
     ]
    }
   ],
   "source": [
    "# Redoing the previous movies request with Pydantic python defining the output structure.\n",
    "\n",
    "import importlib\n",
    "import movies\n",
    "importlib.reload(movies)\n",
    "\n",
    "# Generate a movie review\n",
    "message = movies.generate_movie_review(\"Dirty Dozen\")\n",
    "review = message.parsed\n",
    "\n",
    "# Debug print\n",
    "#movies.print_review_debug(review)\n",
    "\n",
    "# Access individual fields\n",
    "print(f\"Movie Title: {review.title}\")\n",
    "print(f\"Rating: {review.rating}/10\")\n",
    "print(f\"Summary: {review.summary}\")\n",
    "print(f\"Cast:\")\n",
    "for actor in review.cast:\n",
    "    print(f\"\\t- {actor.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
